\chapter{Theoretical Foundations}
\label{chap:preliminaries}
\section{Mathematical Foundations}
\label{sec:Maths}
\begin{figure}
\label{fig:problemLatice}
\includegraphics[width=\textwidth]{./pics/ProblemLatice.pdf}
\caption{Generalization and specialisation dependencies between different problems}
\end{figure}
When modelling we always need ways to express facts in mathematical terms. E.g from physic we know equations that express the relations between different physical values. We are often modelling in order to derive an answer to some particular question. If we are lucky there exists an algorithm and even a software implementation that can compute the answer to our question. In section \ref{sec:atcg2Ampl} we will transform an activity into a mathematical program. Depending on the input model the resulting program will be an instance of one of the problems presented in this section. Currently existing solvers are always specialized on a few problems. When selecting a solver to use with a model one needs to understand of which problem this model is an instance.\\
For some problems like the linear program (\ref{sec:MathLinearProgram}) there exist polynomial time algorithms that can solve every instance of that problem. An algorithm with polynomial runtime is considered a \emph{feasible} algorithm.  For other problems there can be instances that are NP-complete e.g. the boolean satisfiability (\ref{sec:MathBooleanSat}). Consequently an algorithm solving all instances of this problem is \emph{infeasible}. Algorithms are feasible or infeasible. \\
Not every problem presented here will be \emph{decidable}. When a problem is \emph{undecidable} that means there can never be an algorithm solving all instances of this problem. Still there exist heuristic algorithms that are trying to handle undecidable problems, but they may find a solution if they are lucky, or just terminate without finding a solution although one exists, or they might run forever, or even just return a wrong solution. Those heuristics may be feasible or infeasible. \\
One needs to understand those two properties in order to know what one can expect from a solver when trying to solve an instance of a problem. If the used algorithm is feasible, we know that we can expect an answer from the used algorithm even for large problems after a moderate amount of time. For infeasible algorithms the runtime for larger problem instances might grow exponentially. \\
When the problem is decidable we can hope that the used solver is exact and will only tell that the problem has no solution if there really is no solution, and if it returns a solution we assume it is correct. For instances of undecidable problems an algorithm telling that there is no solution just means, that this particular search was not lucky, and potentially returned solutions might not be correct solutions. If a heuristic was not lucky one could start again with different parameters for the search algorithm as for example another starting point, or use another algorithm/solver. In practice one would do this until a certain time limit is up or one is no more interested in the solution.\\
In the following we will introduce a variety of different problems and shortly mention their implications for solvability. In figure \ref{fig:problemLatice} we see an overview of the problems and visualized which problems are generalizations or specialisations of each other. An arrow from A to B means that B is a generalization of A. In green are those problems that are decidable and have at least one feasible algorithm solving all instances. Those are the nice problems in practice.
\subsection{Constraint Programming}
\begin{figure}
\label{fig:CSPExample}
\includegraphics[width=0.5\textwidth]{./pics/SetIntersection.pdf}
\caption{Two dimensional search space and three different subsets forming a constraint satisfaction problem}
\end{figure}
In a \emph{constraint satisfaction problem} (CSP) the task is to assign values $v(x_i)$ to a set of variables $X = (x_1, \dots , x_n)$ from a free search space $S=D_1\times \dots \times D_n$ such that all relations $c_1,\dots,c_k$ between the variables hold. We call those relations $c_1,\dots,c_k$ \emph{constraints}. $D_i$ is called the domain of $x_i$, $n$ is the number of variables and $k$ the number of constraints. Mathematically we can formalise a constraint satisfaction problem as shown in equations (\ref{CSP})-(\ref{CSPEnd}). \cite{Eiben97constraintsatisfaction}\cite{wiki:CSP}.
\begin{eqnarray} 
\label{CSP}
c_i \subseteq S \qquad\forall i \in \left[ 1 \dots k \right]\\
\text{find:} \quad v(x_i) \in D_i \qquad\forall i \in \left[ 1 \dots n \right]| \\
\label{CSPEnd}
\text{subject to:} \quad (v(x_1),\dots , v(x_n)) \in c_j\qquad\forall j \in \left[1 \dots k\right]
\end{eqnarray} 
Any point in $S$ fulfilling all constraints is called a \emph{feasible solution}. The set of all feasible solutions is called the \emph{feasible set}. The feasible set is the intersection of all constraints. If the feasible set is the empty set we call the problem infeasible. Solving a constraint satisfaction problem means finding one feasible solution. The corresponding decision problem denotes whether an algorithmic solution exists or not.\\
Practically constraints are often specified as boolean terms over a subset of $X$ e.g. $x_1=x_2$ or even just one variable e.g. $0\leq x_1 \leq 1$. It is possible to extend any relation between $x_1$ and $x_2$ to a n-ary relation between $x_1,\dots,x_n$. In the extended relation all possible values for the variables not contained in the term specifying a relation are admitted.\\
Note also that there are in general no restrictions on the domains of the variables and the search space. It could be continuous, discrete, finite and infinite. Well known domains are integer, real, boolean but also city names or all triangles similar to an equilateral triangle could form a domain. In other literature about constraint programming often finite discrete search spaces are assumed \cite{Citation Needed}. In fact on a computer an unlimited real number is stored in IEEE 754 floating point format, which is a finite discretization of the real numbers. Also the constraints can have arbitrary properties. Even a fractal set would be acceptable as constraint.\\
In figure \ref{fig:CSPExample} we can see a two dimensional continuous search space and three sets. One that could be assembled by a union of intersections of halfspaces, another that has an arbitrary shape and the last set consists of discrete points marked by the crosses. Any of the points within the intersection of those three sets would be a solution to the corresponding CSP.\\
There is no way to handle all possible ways to describe the constraints by one single algorithm. Additionally this general form includes also a lot of problems that are known to be undecidable or just infeasible to compute. Consequently we need to look at several specializations.
%A constraint $c_j$ is a pair $<t_j,R_j>$ of a set of variables $t_j$ and a relation $R_j$. $t_j$ is a subset of X and R is defined over those variables.

\subsection{Mathematical Programming}
A \emph{Mathematical Program} or \emph{Optimization Problem} is a constraint satisfaction problem with a real valued objective function $f:S\mapsto \mathbb{R}$ and its constraints are specified in terms of functions $g_1,\dots,g_k:S\mapsto\mathbb{R}$. We call those functions constraint functions. The search space is $\mathbb{R}^n$. The task is to find any feasible point that minimizes the objective function $f$.
\begin{eqnarray}
\text{minimize:} \quad f(X)\\
\text{subject to:} \quad g_i(X)\leq 0 \qquad \forall i\in\left[1,\dots ,k\right] 
\end{eqnarray}
\begin{definition}[local optimum]
A local optimum is a feasible solution that minimizes the objective function within a neighbouring set of feasible solutions.
\end{definition}
\begin{definition}[global optimum]
A global optimum is a feasible solution that gives the minimal value for the objective function among all feasible solutions.
\end{definition}
In practice also equations are used as constraints. A $\geq$ can be replaced by $\leq$ or the right hand side is not necessarily $0$ but an arbitrary constant. All those alternative formulations are equivalent to the given formulation.
In this thesis only linear objective functions are considered. For linear objective functions that are not constant any optimum can be found on the boundaries of the feasible set. That is why we will use the word \emph{boundary value} as an equivalent for local optimum.\\
This problem formulation still includes a lot of pathological cases, but there are algorithms as well as software implementations that try to solve problems of this kind. Algorithms trying to solve this problem include heuristic search and convex approximation. The corresponding decision problem whether a solution exists or not is known to be undecidable for this class of problems consequently there can not be a deterministic algorithm always finding a solution or reporting that the problem is infeasible.
\subsection{Convex Optimization}
While there are no efficient algorithms for all optimization problems there do exist very successful methods for solving convex optimization problems\cite{Boyd04ConOpt}.
\begin{definition}[convex set]
A set $C$ in a vector space $S$ is said to be convex if, for all points $x$ and $y$ in $C$ and all $t\in\left[0,1\right]$, the point $(1-t)x+ty$ is in $C$.
\end{definition}
\begin{definition}[convex function]
We call a function f convex iff for any two values $x$ and $y$ the in-equation $ f(\theta x + (1-\theta) y)\leq \theta f(x)+(1-\theta) f(y)$ for $\theta\in \left[0,1\right] $
\end{definition}
A convex optimization problem is a mathematical program with a convex objective function and convex constraint functions. The intersection of convex sets is a convex set itself and the set ${x\in\mathbb{R}^n|g_i(x)\leq 0}$ is a convex set. Consequently the feasible set is also convex. For convex problems every local optimum is a global optimum.\\
Convex optimization problems are particularly efficient to solve with iterative descend methods like quasi newton or interior point methods.
\subsection{Linear Programs}
\label{sec:MathLinearProgram}
Within the class of convex problems there are a lot of special cases. We will only consider one special case within the convex programs, those are \emph{linear programs} (LP).
In linear programs every constraint can be represented as linear inequation and the objective function is an affine function. 
The canonical form of a linear program consists of a matrix $\mathbf{A}$ and two vectors ${c}$ and ${b}$ such that the equations \ref{linDef}f formalize the LP. The $\leq$ is evaluated component wise.
\begin{eqnarray}
\label{linDef}
\text{minimize:}\quad {c}^TX \\
\text{subject to:}\quad AX\leq{b}
\end{eqnarray}
By removing the objective function or setting ${c}={0}$ the problem is relaxed to a linear constraint satisfaction problem. Formulations containing also equations in the constraints on some components of $X$ can be transformed to an equivalent problem in the canonical form. Also a $\max$ instead of $\min$ and a $\geq$ instead of $\leq$ can be used. In practice we will use many possible formulations.\\

An example for a linear program is given in equations \ref{linExample}f.
\begin{eqnarray}
\label{linExample}
\begin{pmatrix}
1 & -2 & 3 \\
4 & 5 & 6 
\end{pmatrix}\times\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix} = \begin{pmatrix}
1 \\ 4
\end{pmatrix}\\
\begin{pmatrix}
0&-1&0
\end{pmatrix}\times\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}\leq \vec{0}
\end{eqnarray}

For constraint satisfaction and constrained optimization with linear constraints and objective function usually an implementation of the simplex algorithm\cite{dantzig63Simplex} is used to solve it very efficiently. Although its worst case runtime is exponential the average runtime grows linear with the number of constraints.

\subsection{Boolean Satisfiability Problems}
In a classic boolean satisfiability problem (SAT) all variables are within the boolean domain and can take the values $true$ and $false$ or $0$ and $1$ respectively. The constraint relations are expressed by a boolean formula. The variables in a SAT problem are also called literals. The task is to find an assignment to the literals, that makes the formula true. A simple example for a boolean formula is given in \ref{boolFormula}.
\begin{eqnarray}
x_1 \land \neg x_2  \lor \neg x_1 \land {x_2}
\label{boolFormula}
\end{eqnarray}
Any boolean formula can be normalized to the disjunctive normal form (DNF). For SAT there is DPLL\cite{DPLL} one of the most efficient algorithms. There are instances of SAT that are NP-complete and thus are in practice infeasible to solve although the corresponding decision problem is always decidable and DPLL is deterministic.\\
An important extension of the SAT problem is the satisfiability modulo theories (SMT) problem. Here instead of literals also sentences that evaluate to true or false in some other theory can appear. Popular backround theories used for SMT are the free theory, linear arithmetic or the theory of bit vectors. In the equations\ref{eqn:SMTExample}f we see an example formula containing propositions in linear arithmetic, Integer arithmetic and one boolean literal.
\begin{eqnarray}
\label{eqn:SMTExample}
x_1,x_2\in \mathbb{R} x_3 \in \mathbb{Z} x_4\in \mathbb{B}\\
(x_1\leq 5) \land (-x_2\leq 10) \lor (x_3=2) \land x_4
\end{eqnarray}
The practicality of SMT problem formulations depend on the used background theories. The famous SMT solver CVC currently implements a large number of logical theories and their combinations\cite{cvc}.

\subsection{Properties of the Search Space}
\subsubsection{Integer Programming}
When all variables in a CSP are from a discrete domain such as finite sets or natural numbers we call it an \emph{integer program} IP. Most integer programs are NP-complete. Famous instances are the discrete logarithm problem or factorization of numbers with only two prime factors. A constraint satisfaction problem or constrained optimization problem where at least one of the domains $D_i$ is discrete among other continuous domains is called a mixed integer problem (MIP). While all linear and most convex optimization problems with only continuous domains can be solved in polynomial time, the introduction of variables with discrete domains often makes the problem much more complex. A linear program with only discrete domains is called \emph{linear integer program} (LIP) and it is a \emph{mixed linear integer program} (MILP) when it has both discrete and continuous domains. We will consider an IP or MIP whose continuous relaxation is a convex problem a \emph{convex mixed integer program}. The continuous relaxation of a mixed inter program is the mathematical program where all discrete
domains are replaced by corresponding continuous domains e.g. the set 
$\left\lbrace 1,2,3 \right\rbrace $
 is replaced by the interval 
 $\left[ 1,3 \right] $
 .\\
An example of an convex mixed integer problem can be given
\begin{eqnarray}
x_1,x_2\in \mathbb{Z} x_3\in \mathbb{R}\\
\text{minimize:}\quad x_3 \\
\text{subject to:}\quad x_1^2 + x_2^2 - 10.5 + x_3 \leq 0 \\
\text{subject to:}\quad x_3 \geq 0
\end{eqnarray}
one possible solution to this problem is $x_1=1$ and $x_2=3$ and the optimal value for $x_3$ is $0.5$. Without the constraint $x_1$ and $x_2$ being Integers it would be easy to find assignments for those variables such that $x_3$ is $0$.\\
Strategies to solve mixed integer programmes include solving its continuous relaxation and do some sort of backtracking as e.g in branch and bound or branch and cut.
\subsubsection{Finite Search Spaces}
When each domain is a finite set then the search space is also finite. In this case at least theoretically one can always find a solution or deny the existence of a solution after exhaustive search. In practice it can still happen that the search space is finite but too large for exhaustive search.\\
%The class of mathematical programs contains instances whose decision problems are undecidable.
%In general CSPs with infinite domains and nonlinear algebraic constraints are undecidable. 
%There are some heuristic algorithms capable of solving many instances of those problems. There exist problem instances for which algorithms tend to run forever. 
One option to break down problems with continuous or infinite search space is reducing its search space to a finite discrete set of possible values. This excludes the largest portion of the search space but makes the problem handleable with existing constraint programming systems. This technique is called \emph{bounded model checking}. This concept matches pretty well with the fact that a computer can only encode finite sets. It is suitable to find a solution if there is one but if it does not find a solution it is not a guarantee that the original problem does not have a solution.\\
Finite constraint programs are usually solved by reducing the domains of variables by constraint propagation and backtracking. Examples for working constraint programming systems are B-Prolog and SWI-Prolog\cite{citation needed}. A well known bounded model checker is SMV\cite{citation needed}. 


%\input{uml.tex}

\section{Coverage Criteria}
\section{taxonomy of Model Based Testing}
Test Model,
Unit Test,
verdict,
Test data,
SUT,
Harness/ stubs,
declarative specification

\section{Preliminary Work}
\label{sec:RelatedWork}
\subsection{executing OCL}
A lot of research in the area of OCL and executing UML/OCL specifications has been performed by Matthias P. Krieger\cite{krieger2008executing}. Also Alloy and Dresden OCL or Eclipse OCL are projects that aim at executing and evaluating OCL specifications. 
\\
But solving OCL constraints as a CSP is only one part of generating test data from a UML Behaviour.
It is very common to use statemachines as the testmodel less papers are handling the generation of unit tests from UML Activities\cite{Linzhang04GeneratingTestCasefromActivityGrayBoxMethod}
\cite{Patel12TestCaseFormationUsigUMLActivityDiagram}
\cite{Pechtanun12GeneratingTestCaseFromUMLActivityDiagramBasedOnACGrammar}
\cite{Xu09ModelCheckingUMLActivities}\cite{Xu09ModelCheckingUMLActivityDiagramsFDR}. 


\subsection{Coverage Criteria}


\subsection{Generating Unit Tests}
Mark Uttig

Jeff Offut

Many papers suppose the use of state of the art SMT solvers such as HOL or CVC to solve OCL Postconditions, Preconditions and  of Operations and invariants embededto get possible inputvalues for functions that can be used as 
The only tool direktly targeted at generating Unit Tests form A UML specification with OCL constraints that was freely available was ParTeG by Stephan Weißleder. In his Ph.D thesis Stephan Weißleder describes a framework for applying different control flow based as well as partition based coverage criteria to state machines. He also develops a theory how all those coverage criteria are interconnected and can be simulated by each other and a transformed version of the original Test Model\cite{ParTeG}.


tell about other test generating tools (commercial, published in Papers) and ParTeG as the only one where the source code is available.\cite{ParTeG}

\section{The AMPL Modelling System}
\label{sec:AMPL}
\cite{AMPL}
