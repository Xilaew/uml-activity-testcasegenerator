\chapter{Theoretical Foundations}
Throughout this thesis we will develop a complicated algorithm. We will relay on existing technology and knowledge throughout the third and the fourth chapter. Thus in this chapter we present a minimal mindset that is helpful to understand the presented algorithm and reproduce our argumentation. The first thing we need is a little background knowledge on the computability of mathematical problems and the algorithms that are applied to the considered problems. The second technology we need prior knowledge about is the \textbf{U}nified \textbf{M}odelling \textbf{L}anguage (UML) and the \textbf{O}bject \textbf{C}onstraint \textbf{L}anguage, since those are the input languages for the presented algorithm. \textbf{A} \textbf{M}athematical \textbf{P}rogramming \textbf{L}anguage (AMPL) is used throughout the presented algorithm thus we will explain the basic concepts of AMPL in this chapter. Further it is also necessary to shortly elaborate on the taxonomy in the field of Model--Based Testing to avoid confusions. Finally, we will present in the section \nameref{sec:RelatedWork} what others did in order to solve similar problems as the one we are going to solve.
\label{chap:preliminaries}
\section{Mathematical Foundations}
\label{sec:Maths}
\begin{figure}
\label{fig:problemLatice}
\includegraphics[width=\textwidth]{./pics/ProblemLatice.pdf}
\caption{Generalisation and specialization dependencies between different problems}
\end{figure}
%When modelling we always need ways to express facts in mathematical terms. E.g from physic we know equations that express the relations between different physical values. 
We are often modelling in order to derive an answer to some particular question. If we are lucky there exists an algorithm and even a software implementation that can compute the answer to our question. A major task in this thesis is to transform an activity diagram to a mathematical program, that is, a constraint satisfaction problem, and to apply a constraint solver. Currently existing solvers are always specialized on a few problems. When selecting a solver one needs to understand of which problem this model is an instance. Depending on the input model the resulting mathematical program will be an instance of one of the problems presented in this section.\\
For some problems like the linear program (\ref{sec:MathLinearProgram}) there exist polynomial time algorithms. An algorithm with polynomial worst case runtime is considered a \emph{tractable} algorithm. For other problems there can be instances that are NP-complete for example the boolean satisfiability (SAT) presented in section \ref{sec:MathBooleanSat}. Any algorithm solving an NP--complete problem will be \emph{intractable}. We consider an algorithm tractable when its worst case runtime is polynomial and intractable when its worst case runtime is exponential.\\
Not every problem presented here will be \emph{decidable}. When a problem is \emph{undecidable} that means there can never be an algorithm solving all instances of this problem. Still there exist heuristic algorithms that try to handle undecidable problems. They may find a solution if they are lucky, or just terminate without finding a solution although one exists, or they might run forever, or even just return a wrong answer. Those heuristics may be tractable or intractable.\\
One needs to understand those two properties, tractability and decidability, in order to know what one can expect from a solver when trying to solve an instance of a problem. If the used algorithm is tractable, we know that we can expect an answer even for large problems after a moderate amount of time. For intractable algorithms the runtime might grow exponentially and thus make it practically infeasible to compute an answer to the problem.\\
When the problem is decidable we can assume that the used solver is exact and will tell that the problem has no solution only if there really is no solution, if it returns a solution we assume it is correct. For instances of undecidable problems a heuristic telling that there is no solution just means, that this particular search was not lucky, and potentially returned answers might not be correct solutions of the given problem. If a heuristic was not lucky and did not find a potential solution one could start again with different parameters for the search algorithm, for example, another starting point, or could use another solver. Typically we are asking for an optimal solution and a heuristic will either find the true optimum, or return a good and practically useful but sub--optimal and thus incorrect answer, or it reports that no possible answer could be found. In practice one will run a mathematical search until a certain time limit is up.\\
In the following we will introduce those constraint satisfaction problems that we think are relevant to generate test data from activity diagrams in practice. We will shortly mention their implications for solvability. In Figure \ref{fig:problemLatice} we give an overview of the problems and visualise which problems are generalisations or specializations of each other. An arrow from A to B means that B is a generalisation of A. In green are those problems that are decidable and have at least one tractable algorithm. Those are the easy--to--solve--problems in practice. All the other problems listed are challenging.
\subsection{Constraint Programming}
\begin{figure}
\label{fig:CSPExample}
\includegraphics[width=0.5\textwidth]{./pics/SetIntersection.pdf}
\caption{Two--dimensional search space and three different subsets forming a constraint satisfaction problem}
\end{figure}
In a \emph{constraint satisfaction problem} (CSP) the task is to assign values $v(x_i)$ to a set of variables $X = (x_1, \dots , x_n)$ from a search space $S=D_1\times \dots \times D_n$ such that all relations $c_1,\dots,c_k$ between the variables hold. We call those relations $c_1,\dots,c_k$ \emph{constraints}. $D_i$ is called the domain of $x_i$, $n$ is the number of variables, and $k$ the number of constraints. We can formalise a constraint satisfaction problem as shown in equations (\ref{CSP})--(\ref{CSPEnd}) \cite{Eiben97constraintsatisfaction}\cite{wiki:CSP}.
\begin{eqnarray} 
\label{CSP}
c_i \subseteq S \qquad\forall i \in \left[ 1 \dots k \right]\\
\text{find:} \quad v(x_i) \in D_i \qquad\forall i \in \left[ 1 \dots n \right]| \\
\label{CSPEnd}
\text{subject to:} \quad (v(x_1),\dots , v(x_n)) \in c_j\qquad\forall j \in \left[1 \dots k\right]
\end{eqnarray} 
Any point in $S$ fulfilling all constraints is called a \emph{feasible solution}. The set of all feasible solutions is called the \emph{feasible set}. The feasible set is the intersection of all constraints. If the feasible set is the empty set we call the problem infeasible. Solving a constraint satisfaction problem means finding one feasible solution.\\
Practically, constraints are often specified as boolean terms over a subset of $X$. For example $x_1=x_2$ specifies a constraint. A constraint can also be specified by only one variable as in $0\leq x_1 \leq 1$. It is possible to extend any relation between $x_1$ and $x_2$ to a n-ary relation between $x_1,\dots,x_n$. In the extended relation all possible values are permitted for the variables not contained in the term.\\
In general, there are no restrictions on the domains of the variables and the search space. They could be continuous, discrete, finite and infinite. Well known domains are integer, real, boolean but city names or all triangles similar to an equilateral triangle could also form a domain. Similarly, the constraints can have arbitrary properties. Even a fractal set would be acceptable as constraint. In literature about constraint programming often finite discrete search spaces are assumed \cite{Citation Needed}. In fact, on a computer, a real number is stored in IEEE 754 floating point format, which is a finite discretization of the real numbers. \\
In Figure \ref{fig:CSPExample} we can see a two--dimensional continuous search space and three sets. One that could be assembled by a union of intersections of half--spaces, another that has an arbitrary shape, and the last set that consists of discrete points marked by the crosses. Any of the points within the intersection of those three sets is a solution to the corresponding CSP.\\
The CSP in its general form is undecidable but with restrictions on the kind of constraints and domains it becomes decidable. For example a CSP with all variables in $\mathbb{B}$ is decidable. We therefore reduce our expectations and consider those specializations, for which algorithms exist. Among the decidable specializations of a CSP those with a tractable algorithm are especially easy--to--solve. Whenever it is possible to state a certain problem as an instance of such an easy--to--solve problem one can be sure that there is a suitable solver solving the problem correct in a moderate amount of time.\\
In practice it is not always easy or impossible to state a question as an instance of a decidable problem having a tractable algorithm. So we will also consider decidable problems with intractable algorithms as well as undecidable problems with tractable and intractable heuristics. A solver is still useful in practice if the worst case runtime is exponential but only for some specially crafted problem instances the worst case happens.%The simplex algorithm is a good example for such an algorithm. Its worst case runtime is exponential but for problems occurring in practice its runtime grows linear with the number of constraints.
For practical problems it is often the case, that an intractable algorithm has a non--exponential average runtime. 
For undecidable problems there exist heuristics that give useful answers to a given problem instance. The answer of a heuristic may not be perfectly correct for example it can return a sub-optimal point for an optimisation. In most cases the answer of a heuristic is good enough to be useful in practice. We are especially interested in specializations of the CSP that impose as little constraints on the domains and constraint formulations as possible and are just specialized enough that we can find a solver that reports useful answers for practical instances after a moderate amount of time. Those are the user--friendly specializations fo the CSP, since the user stating the problem does not need to care for to many restrictions on the way he formulates the problem.
%There is no way to handle all possible ways to describe the constraints by one single algorithm. Additionally this general form includes also a lot of problems that are known to be undecidable or just infeasible to compute. Consequently we need to look at several specializations. //BULLSHITT!!!!
%A constraint $c_j$ is a pair $<t_j,R_j>$ of a set of variables $t_j$ and a relation $R_j$. $t_j$ is a subset of X and R is defined over those variables.

\subsection{Mathematical Programming}
A \emph{Mathematical Program} or \emph{Optimisation Problem} (OP) is a constraint satisfaction problem with a real-valued \emph{objective function} $f:S\mapsto \mathbb{R}$ where constraints are specified in terms of functions $g_1,\dots,g_k:S\mapsto\mathbb{R}$. We call those functions \emph{constraint functions}. The search space is $\mathbb{R}^n$. The task is to find a feasible point that minimises the objective function $f$.
\begin{eqnarray}
\text{minimise:} \quad f(X)\\
\text{subject to:} \quad g_i(X)\leq 0 \qquad \forall i\in\left[1,\dots ,k\right] \label{eqn:MOPs.t.}
\end{eqnarray}
We distinguish two kinds of optima, the local optimum and the global optimum. Global optima are in general much more difficult to find than local optima.
\begin{definition}[local optimum]
A local optimum is a feasible solution that minimises the objective function within a neighbouring set of feasible solutions.
\end{definition}
\begin{definition}[global optimum]
A global optimum is a feasible solution that gives the minimal value for the objective function among all feasible solutions.
\end{definition}
In practice equations are also used as constraints. In Equation (\ref{eqn:MOPs.t.}), $\leq$ is permitted as well as an arbitrary constant on the right--hand side. \\
In this thesis only linear objective functions are considered. For linear objective functions that are not constant any optimum can be found on the boundaries of the feasible set. That is why the word \emph{boundary value} will be used as an equivalent for local optimum. Using a constant objective function relaxes the problem, since with a constant objective function every feasible point is a global optimum.\\
The OP is undecidable, but there are very successful heuristics available. One way to tackle an OP is to add quadratic penalty terms to the objective function, which are growing when a constraint is violated. Then one selects an arbitrary starting point and applies a descend method such as Newton's method to find a local minimum of the augmented objective function. One can try this local search with a descend method multiple times from different starting points and then select among the points found the one that gives the lowest value for the objective function. With a high likelihood, this algorithm will find a feasible point, and the global optimum could have been among the computed local optima.
\subsection{Convex Optimisation}
The \emph{convex optimisation problem} (COP) is one specialization of the optimisation problem for which tractable algorithms exist \cite{Boyd04ConOpt}.
\begin{definition}[convex set]
A set $C$ in a vector space $S$ is said to be convex if, for all points $x$ and $y$ in $C$ and all $t\in\left[0,1\right]$, the point $(1-t)x+ty$ is in $C$.
\end{definition}
\begin{definition}[convex function]
A function f is convex iff for any two values $x$ and $y$ the inequality $ f(\theta x + (1-\theta) y)\leq \theta f(x)+(1-\theta) f(y)$ with $\theta\in \left[0,1\right] $ holds.
\end{definition}
A convex optimisation problem is a mathematical program with a convex objective function and convex constraint functions. The intersection of convex sets is a convex set itself and the set $\left\lbrace x\in\mathbb{R}^n|g_i(x)\leq 0\right\rbrace$ is a convex set. Consequently the feasible set is also convex. For convex problems every local optimum is a global optimum.\\
The convex optimisation problem is decidable and additionally there are tractable interior point methods solving instances of that problem. Interior point methods first need a feasible point as starting point. The objective function is augmented by logarithmic barrier functions for each constraint with a form factor to steer the steepness of these barriers. Now a descend method is applied to find the lowest point. Then repeatedly the barriers are made steeper and the last point found is used as a new starting point. The repetition ends when a stop criterion is fulfilled.
\subsection{Linear Programming}
\label{sec:MathLinearProgram}
Within the class of convex optimisation problems there exist many special cases. One specialization of convex programming is \emph{linear programming} (LP).
In a linear program every constraint can be represented as linear inequality and the objective function is an affine function. 
The canonical form of a linear program consists of a matrix $\mathbf{A}$ and two vectors ${c}$ and ${b}$ as shown in the equations (\ref{linDef})-(\ref{linDefEnd}). The relation $\leq$ is evaluated componentwise.
\begin{eqnarray}
\label{linDef}
\text{minimise:}\quad {c}^TX \\
\label{linDefEnd}
\text{subject to:}\quad AX\leq{b}
\end{eqnarray}
By removing the objective function or setting ${c} = {0} $ the problem is relaxed to a linear constraint satisfaction problem. Formulations containing also equations in the constraints on some components of $X$ can be transformed to an equivalent problem in the canonical form. Also, maximisation of the objective function might be required instead of a minimisation. Further the relation $\geq$ might replace the $\leq$ for some components of the constraint. In practice we will use all of these formulations.\\
An example of a linear program is given in equations (\ref{linExample})-(\ref{linExampleEnd}).
\begin{eqnarray}
\label{linExample}
\begin{pmatrix}
1 & -2 & 3 \\
4 & 5 & 6 
\end{pmatrix}\times\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix} = \begin{pmatrix}
1 \\ 4
\end{pmatrix}\\
\begin{pmatrix}
0&-1&0
\end{pmatrix}\times\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\label{linExampleEnd}
\end{pmatrix}\leq \vec{0}
\end{eqnarray}
For constraint satisfaction and constrained optimisation with linear constraints and objective function usually an implementation of the simplex algorithm \cite{dantzig63Simplex} can to solve it very efficiently. Although its worst case runtime is exponential the average runtime grows linear with the number of constraints. For a geometric interpretation of the simplex algorithm we interpret the constraints as a set of hyper--planes in an $n$--dimensional space. The feasible region is a polyeder whose faces are within the hyper--planes defined by the constraints. The simplex algorithm starts at an arbitrary exposed point of this polyeder and then in moves along one edge to another exposed point with a better objective value. This step is repeated until the exposed point with the minimal or maximal objective value is found.
\subsection{Boolean Satisfiability Problems}
\label{sec:MathBooleanSat}
In a classic boolean satisfiability problem (SAT) all variables are within the boolean domain and can take the values $true$ and $false$ or $0$ and $1$ respectively. The constraint relations are expressed by a boolean formula.  The task is to find an assignment to the variables that makes the formula true. A simple example of a boolean formula is given in equation (\ref{boolFormula}).
\begin{eqnarray}
x_1 \land \neg x_2  \lor \neg x_1 \land {x_2}
\label{boolFormula}
\end{eqnarray}
Any boolean formula can be normalised to a conjunction of disjunctive clauses, the so--called \emph{conjunctive normal form} (CNF). The boolean satisfiability problem is decidable, but belongs to the class of NP-complete problems. Thus algorithms solving this problem are intractable. However, there are specializations of the SAT problem for which a solution can be computed in polynomial time, for example 2-SAT where each disjunctive clause in the CNF consists of at most 2 \emph{literals}. A literal can either be a variable or the negation of a variable.\\
An important generalisation of the SAT problem is the \emph{satisfiability modulo theories} (SMT) problem. Here, instead of literals, sentences can appear that evaluate to true or false in some theory. Common background theories used for SMT are the free theory, linear arithmetic, or the theory of bit vectors. In the equations (\ref{eqn:SMTExample})-(\ref{eqn:SMTExampleEnd}) we see an example formula containing propositions in linear arithmetic, integer arithmetic, and one boolean literal.
\begin{eqnarray}
\label{eqn:SMTExample}
x_1,x_2\in \mathbb{R}, \quad x_3 \in \mathbb{Z},\quad x_4\in \mathbb{B}\\
\label{eqn:SMTExampleEnd}
(x_1\leq 5) \land (-x_2\leq 10) \lor (x_3=2) \land x_4
\end{eqnarray}
The decidability of SMT problem formulations depends on the background theories used. If the SMT is restricted to a set of decidable background theories then the SMT is decidable. If undecidable theories such as non linear arithmetic including transcendent functions are allowed then the SMT is undecidable. The well--known SMT solver CVC currently implements a large number of logical theories and their combinations \cite{cvc}\\
DPLL \cite{DPLL} is a classic algorithm solving SAT problems. The DPLL algorithm combines a backtracking search and local consistency checks. In the backtracking step one literal is selected and the problem splits up in two sub--problems, one in which the selected literal is assigned true and one, where it is set to false. All disjunctive clauses that become true by the assignment can be removed from the formula, and from those clauses that do not evaluate to true the assigned literal is removed. When there is a clause consisting of only one literal this literal can only be assigned in one way to make that clause true. In unit propagation this literal will be assigned and all clauses containing that literal can be removed from the formula. From all clauses containing the opposite literal the literal is removed. Unit propagation is repeated until there are no more clauses containing a single literal. Next pure literals are eliminated. A pure literal is a literal that is only present with one polarity. When a variable is for example only occurring as negative literal one can assign that variable to false and all clauses containing the negative literal are evaluating to true and can be removed. If in the end there is one empty clause, that is, a clause for which all literals are assigned and evaluated to false, then the current sub--problem is not satisfiable and one has to backtrack. If there are no more clauses left that means for the current assignment of literals every clause contains at least one literal evaluating to true then a satisfying assignment for the variables has been found.
%There are instances of SAT that are NP-complete and thus are in practice infeasible to solve although the corresponding decision problem is always decidable and DPLL is deterministic.\\
\subsection{Properties of the Search Space}
Every specialization of the CSP is imposing constraints on the formulation of constraints and on the selection of domains. For the problems presented so far, we focused on the ways constraints are stated. In this section we present further problems that can be obtained by strengthening or relaxing the constraints imposed on the selection of domains.
\subsubsection{Integer Programming}
When all variables in an CSP are in the discrete domain of natural numbers we call the CSP an \emph{integer program} (IP). Most integer programs are NP--complete. Well--known instances are the discrete logarithm problem and the factorisation of numbers with only two prime factors. A constraint satisfaction problem with variables in the domain of natural numbers as well in the domain of real numbers is called a \emph{mixed integer problem} (MIP). Optimisation problems, convex programs, and linear programs can be generalised by adding variables in the domain of natural numbers. While all convex optimisation problems with only continuous domains can be solved in polynomial time, the introduction of variables with discrete domains often makes the problem much more complex. A linear program with only discrete domains is called \emph{linear integer program} (LIP) and it is called a \emph{mixed linear integer program} (MILP) when it has both discrete and continuous domains. We call an IP or MIP whose \emph{continuous relaxation} is a convex problem a \emph{convex mixed integer program}. The continuous relaxation of a mixed integer program is the mathematical program where all discrete domains are replaced by corresponding continuous domains. For example, the set 
$\left\lbrace 1,2,3 \right\rbrace $
 could be replaced by the interval 
 $\left[ 1,3 \right] $
 .\\
An example of the convex mixed integer problem is as follows:
\begin{eqnarray}
x_1,x_2\in \mathbb{Z},\quad x_3\in \mathbb{R}\\
\text{minimise:}\quad x_3 \\
\text{subject to:}\quad x_1^2 + x_2^2 - 10.5 + x_3 \leq 0 \\
\text{subject to:}\quad x_3 \geq 0
\end{eqnarray}
One possible solution to this problem is $x_1=1$ and $x_2=3$ and the optimal value for $x_3$ is $0.5$. Without the integral constraint on $x_1$ and $x_2$ the solution to this problem could be computed by an interior point method in polynomial runtime; $x_3$ would then be $0$. With $x_1,x_2\in \mathbb{N}$ one can estimate an upper and a lower bound for $x_1$ and $x_2$ and then needs to try out all combinations of possible values for those two variables.\\
To solve a mixed integer problem a combination of different algorithms is used. One strategy is the \emph{cutting--planes} method. For cutting--planes first the optimal solution of the continuous relaxation is computed. If the optimal solution is integral in all the components that are required to be an integer we are done. If that is not the case a constraint is added such that all feasible points are still within the new feasible set, but the currently optimal solution of the continuous relaxation is excluded. Then, the solution of the continuous relaxation of the new problem is computed. This repeats until no more additional constraint can be found or the solution is integral in those components that are required to be integral. Mathematicians have found a variety of strategies to generate those additional constraints for the cutting--planes method. Another strategy is the branch--and--bound method. In branch--and--bound, the solution of the continuous relaxation is computed first and, in case the integrality constraints are not satisfied, the problem is split into sub--problems excluding the current solution of the continuous relaxation. For example the problem above might be split up into two sub--problems, by one time adding the constraint $x_2\geq 1$ and one time adding $x_2\leq 0$ to the original problem. This separation would exclude all solutions with $x_2$ between $0$ and $1$. The solution of the continuous relaxation, however, gives a lower bound for the optimal value. An upper bound for the optimal value of a sub--problem can be gained by a heuristic. If the lower bound of sub--problem A is above the upper bound for sub--problem B then sub--problem A can be pruned. Thus large portions of the search space are excluded from the search.
\subsubsection{Finite Search Spaces}
When each domain is a finite set then the search space is also finite. We call a constraint satisfaction problem with finite search space \emph{finite constraint satisfaction problem}. For a finite constraint satisfaction problem at least theoretically one can always find a solution or deny the existence of a solution after exhaustive search. In practice the search space is finite but often too large for exhaustive search.\\
%The class of mathematical programs contains instances whose decision problems are undecidable.
%In general CSPs with infinite domains and nonlinear algebraic constraints are undecidable. 
%There are some heuristic algorithms capable of solving many instances of those problems. There exist problem instances for which algorithms tend to run forever. 
%One option to break down problems with continuous or infinite search space is reducing its search space to a finite discrete set of possible values. This excludes the largest portion of the search space but makes the problem handleable with existing constraint programming systems. This technique is called \emph{bounded model checking}. The concept of bounded model checking matches pretty well with the fact that a computer can only encode finite sets. It is suitable to find a solution if there is one but if it does not find a solution it is not a guarantee that the original problem does not have a solution.\\
Finite constraint programs can be solved by reducing the domains of variables by constraint propagation, and local consistency checks, and backtracking when an inconsistency is found \cite{ConstraintPropagation}. Constraint propagation is a kind of logical inference and based on rules for reasoning. For example from the two constraints $x\leq 5$ and $y \leq x$ one can conclude that $y\leq 5$, thus the domain of y has been reduced. Examples of working constraint programming systems are B-Prolog and SWI-Prolog \cite{citation needed}. %A well known bounded model checker is SMV \cite{citation needed}. 
A different approach for solving finite constraint programs is stating the problem as an integer program and solving it with a combination of branch--and--bound and the cutting--planes algorithm.
\input{uml.tex}
\section{The AMPL Modelling System}
\label{sec:AMPL}
\cite{AMPL}
\section{Coverage Criteria}
\section{Taxonomy of Model Based Testing}
Test Model,
Unit Test,
verdict,
Test data,
SUT,
Harness/ stubs,
declarative specification

\section{Preliminary Work}
\label{sec:RelatedWork}
\subsection{Executing OCL}
A lot of research in the area of OCL and executing UML/OCL specifications has been performed by Matthias P. Krieger \cite{krieger2008executingUnderspecifiedOCL}. Also Alloy and Dresden OCL or Eclipse OCL are projects that aim at executing and evaluating OCL specifications. \\
But solving OCL constraints as a CSP is only one part of generating test data from a UML Behaviour.
It is very common to use state machines as the test model less papers are handling the generation of unit tests from UML Activities\cite{Linzhang04GeneratingTestCasefromActivityGrayBoxMethod}
\cite{Patel12TestCaseFormationUsigUMLActivityDiagram}
\cite{Pechtanun12GeneratingTestCaseFromUMLActivityDiagramBasedOnACGrammar}
\cite{Xu09ModelCheckingUMLActivities}\cite{Xu09ModelCheckingUMLActivityDiagramsFDR}. 


\subsection{Coverage Criteria}



\subsection{Generating Unit Tests}
Mark Uttig

Jeff Offut

Many papers suppose the use of state of the art SMT solvers such as HOL or CVC to solve OCL post--conditions, pre--conditions and of operations and invariants embedded to get possible input values for functions that can be used as 
The only tool directly targeted at generating Unit Tests form A UML specification with OCL constraints that was freely available was ParTeG by Stephan Wei\ss leder. In his PhD thesis Stephan Wei\ss leder describes a framework for applying different control flow based as well as partition based coverage criteria to state machines. He also develops a theory how all those coverage criteria are interconnected and can be simulated by each other and a transformed version of the original Test Model\cite{ParTeG}.


tell about other test generating tools (commercial, published in Papers) and ParTeG as the only one where the source code is available.\cite{ParTeG}


